# PDF Question Answering System

This is a question-answering system that allows users to upload PDF files and ask questions based on the content of those files. The system uses the Mistral-7B language model to generate answers by retrieving relevant context from the uploaded documents.

## Features

- Upload multiple PDF files
- Store the text content of the uploaded PDFs in a Weaviate vectorstore
- Ask questions based on the content of the uploaded PDFs
- Get answers generated by the Mistral-7B language model
- Clear the vectorstore and chat history

## Requirements

- Python 3.x
- transformers
- torch
- langchain
- langchain_community
- langchain_core
- gradio
- weaviate

## Installation

1. Clone the repository:

```
git clone https://github.com/your-username/RAG_Mistral_Weaviate_Gradio.git
```

2. Install the required dependencies:

```
pip install -r requirements.txt
```

## Usage

1. Run the script:

```
gradio app.py
```

2. Open the provided URL in your web browser to access the user interface.

3. Upload one or more PDF files using the file upload component.

4. Enter your question in the textbox and submit it to get an answer based on the content of the uploaded PDFs.

5. The chat history will be displayed in the chatbot component.

6. To clear the vectorstore and chat history, click the "Clear VectorDB and Chat" button.

## How it Works

1. The `load_llm()` function loads the Mistral-7B language model using the provided configurations.

2. The `embeddings_model()` function loads the sentence-transformers/all-mpnet-base-v2 embeddings model.

3. The `initialize_vectorstore()` function initializes an empty Weaviate vectorstore using the provided Weaviate URL and the loaded embeddings model.

4. The `text_splitter()` function creates a `RecursiveCharacterTextSplitter` object for splitting text into chunks.

5. The `add_pdfs_to_vectorstore()` function processes the uploaded PDF files, extracts the text content, splits it into chunks, and adds the chunks to the vectorstore.

6. The `weaviate_client()` function creates a Weaviate client using the provided URL.

7. The `answer_query()` function handles user queries by performing a similarity search on the vectorstore to find relevant context, constructing a prompt template with the context and the question, and generating an answer using the language model.

8. The `clear_vectordb()` function clears the vectorstore and chat history when the "Clear VectorDB and Chat" button is clicked.

9. The Gradio user interface is set up using the `gr.Blocks()` context manager, defining the layout and components of the interface.

10. Event handlers are set up for the interface components to handle file uploads, user queries, and clearing the vectorstore and chat history.

## Contributing

Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.

## License

This project is licensed under the [MIT License](LICENSE).