{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vardh/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "AutoTokenizer,\n",
    "AutoModelForCausalLM,\n",
    "BitsAndBytesConfig,\n",
    "pipeline\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, format_document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "\n",
    "    #Loading the Mistral Model\n",
    "    model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "\n",
    "    # Building a LLM text-generation pipeline\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=True,\n",
    "        max_new_tokens=1024,\n",
    "        device_map = 'auto',\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline= text_generation_pipeline)\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_model():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vectorstore():\n",
    "\n",
    "    vectorstore = Weaviate.from_documents(\n",
    "        [], embedding=hf_embeddings,\n",
    "        weaviate_url = 'https://superteams-810p8edk.weaviate.network',\n",
    "        by_text= False\n",
    "    )\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textsplitter():\n",
    "    # Simulate some document processing delay\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_chain_elements():\n",
    "\n",
    "    #template to get the Standalone question\n",
    "    _template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "        Chat History:\n",
    "        {chat_history}\n",
    "        Follow Up Input: {question}\n",
    "        Standalone question:\n",
    "    \"\"\"\n",
    "    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "    #Function to create the context from retrieved documents\n",
    "    DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "    def _combine_documents(\n",
    "        docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "    ):\n",
    "        doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "        return document_separator.join(doc_strings)\n",
    "\n",
    "    #Creating the template for the final answer\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "    \"\"\"\n",
    "    ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "    # Now we calculate the standalone question\n",
    "    standalone_question = {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "        }\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | llm\n",
    "        | StrOutputParser(),\n",
    "    }\n",
    "\n",
    "    # Now we retrieve the documents\n",
    "    retrieved_documents = {\n",
    "        \"docs\": itemgetter(\"standalone_question\") | vectorstore.as_retriever(search_kwargs = {'k':10}),\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "\n",
    "    # Now we construct the inputs for the final prompt\n",
    "    final_inputs = {\n",
    "        \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "\n",
    "    # And finally, we do the part that returns the answers\n",
    "    answer = {\n",
    "        \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "        \"docs\": itemgetter(\"docs\"),\n",
    "    }\n",
    "\n",
    "    return standalone_question, retrieved_documents, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pdfs_to_vectorstore(files):\n",
    "\n",
    "    saved_files_count = 0\n",
    "    for file_path in files:\n",
    "        file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "        if file_name.lower().endswith('.pdf'):  # Check if the file is a PDF\n",
    "            saved_files_count += 1\n",
    "            loader_temp = PyPDFLoader(file_path)\n",
    "            docs_temp = loader_temp.load_and_split(text_splitter=textsplitter)\n",
    "            vectorstore.add_documents(docs_temp)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping non-PDF file: {file_name}\")\n",
    "\n",
    "    return f\"Added {saved_files_count} PDF file(s) to vectorstore/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversational_memory():\n",
    "    return ConversationBufferMemory(\n",
    "        return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(message, chat_history):\n",
    "    loaded_memory = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(conversational_memory.load_memory_variables) | itemgetter(\"history\"),\n",
    "    )\n",
    "    chain = loaded_memory | standalone_question | retrieved_documents | answer\n",
    "\n",
    "    inputs = {\"question\": message}\n",
    "    result = chain.invoke(inputs)\n",
    "\n",
    "    chat_history.append((message, result[\"answer\"]))\n",
    "    conversational_memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
    "\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "llm = load_llm()\n",
    "\n",
    "hf_embeddings = embeddings_model()\n",
    "\n",
    "vectorstore = initialize_vectorstore()\n",
    "\n",
    "standalone_question, retrieved_documents, answer = return_chain_elements()\n",
    "\n",
    "textsplitter = textsplitter()\n",
    "\n",
    "conversational_memory = conversational_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "\n",
    "   with gr.Row():\n",
    "       upload_files = gr.File(label= \"Upload pdf files only\", file_count= 'multiple')\n",
    "       success_msg = gr.Text(value=\"\")\n",
    "\n",
    "   chatbot = gr.Chatbot()\n",
    "   msg = gr.Textbox(label= \"Enter your query here\")\n",
    "   clear = gr.ClearButton([msg, chatbot])\n",
    "\n",
    "\n",
    "   upload_files.upload(add_pdfs_to_vectorstore, upload_files, success_msg)\n",
    "   msg.submit(answer_query, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(server_name='0.0.0.0', share= True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
