{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "AutoTokenizer,\n",
    "AutoModelForCausalLM,\n",
    "BitsAndBytesConfig,\n",
    "pipeline\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, format_document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Loading the Mistral Model\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Building a LLM text-generation pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1024,\n",
    "    device_map = 'auto',\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline= text_generation_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(url=  'https://superteams-810p8edk.weaviate.network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.schema.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    [], embedding=hf_embeddings,\n",
    "    client = client ,\n",
    "    by_text= False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simulate some document processing delay\n",
    "textsplitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#template to get the Standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    Follow Up Input: {question}\n",
    "    Standalone question:\n",
    "\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "#Function to create the context from retrieved documents\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "#Creating the template for the final answer\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | vectorstore.as_retriever(search_kwargs = {'k':10}),\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_pdfs_to_vectorstore(files):\n",
    "\n",
    "    saved_files_count = 0\n",
    "    for file_path in files:\n",
    "        file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "        if file_name.lower().endswith('.pdf'):  # Check if the file is a PDF\n",
    "            saved_files_count += 1\n",
    "            loader_temp = PyPDFLoader(file_path)\n",
    "            docs_temp = loader_temp.load_and_split(text_splitter=textsplitter)\n",
    "            vectorstore.add_documents(docs_temp)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping non-PDF file: {file_name}\")\n",
    "\n",
    "    return f\"Added {saved_files_count} PDF file(s) to vectorstore/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_pdfs_to_vectorstore(['Goddess - Plot.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversational_memory = ConversationBufferMemory(\n",
    "        return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_vectordb(chatbot, msg):\n",
    "    client.schema.delete_all()\n",
    "    conversational_memory.clear()\n",
    "    chatbot = \"\"\n",
    "    msg = \"\"\n",
    "    return chatbot, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_query(message, chat_history):\n",
    "    loaded_memory = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(conversational_memory.load_memory_variables) | itemgetter(\"history\"),\n",
    "    )\n",
    "    chain = loaded_memory | standalone_question | retrieved_documents | answer\n",
    "\n",
    "    inputs = {\"question\": message}\n",
    "    result = chain.invoke(inputs)\n",
    "\n",
    "    chat_history.append((message, result[\"answer\"]))\n",
    "    conversational_memory.save_context(inputs, {\"answer\": result[\"answer\"]})\n",
    "\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = load_llm()\n",
    "\n",
    "hf_embeddings = embeddings_model()\n",
    "\n",
    "vectorstore = initialize_vectorstore()\n",
    "\n",
    "standalone_question, retrieved_documents, answer = return_chain_elements()\n",
    "\n",
    "textsplitter = textsplitter()\n",
    "\n",
    "conversational_memory = conversational_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/routes.py:945: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/fastapi/applications.py:4495: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  return self.router.on_event(event_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://defe2053333ace9cec.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://defe2053333ace9cec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/starlette/templating.py:172: DeprecationWarning: The `name` is not the first parameter anymore. The first parameter should be the `Request` instance.\n",
      "Replace `TemplateResponse(name, {\"request\": request})` by `TemplateResponse(request, name)`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1710, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/utils.py\", line 693, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_6280/2797907116.py\", line 8, in answer_query\n",
      "    result = chain.invoke(inputs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in <dictcomp>\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 248, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 569, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 748, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 606, in _generate_helper\n",
      "    raise e\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 593, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 266, in _generate\n",
      "    responses = self.pipeline(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 241, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1177, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1102, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 328, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1544, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2404, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1157, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1042, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 757, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 302, in forward\n",
      "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 0; 14.58 GiB total capacity; 13.48 GiB already allocated; 581.62 MiB free; 13.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/starlette/templating.py:172: DeprecationWarning: The `name` is not the first parameter anymore. The first parameter should be the `Request` instance.\n",
      "Replace `TemplateResponse(name, {\"request\": request})` by `TemplateResponse(request, name)`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1710, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/utils.py\", line 693, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_6280/2797907116.py\", line 8, in answer_query\n",
      "    result = chain.invoke(inputs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in <dictcomp>\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 248, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 569, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 748, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 606, in _generate_helper\n",
      "    raise e\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 593, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 266, in _generate\n",
      "    responses = self.pipeline(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 241, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1177, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1102, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 328, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1544, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2404, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1157, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1042, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 757, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 285, in forward\n",
      "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB (GPU 0; 14.58 GiB total capacity; 10.96 GiB already allocated; 581.62 MiB free; 13.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "\n",
    "   with gr.Row():\n",
    "       upload_files = gr.File(label= \"Upload pdf files only\", file_count= 'multiple')\n",
    "       success_msg = gr.Text(value=\"\")\n",
    "\n",
    "   chatbot = gr.Chatbot()\n",
    "   msg = gr.Textbox(label= \"Enter your query here\")\n",
    "   clear = gr.ClearButton([msg, chatbot], value= \"Clear VectorDB\")\n",
    "\n",
    "\n",
    "   upload_files.upload(add_pdfs_to_vectorstore, upload_files, success_msg)\n",
    "   msg.submit(answer_query, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(server_name='0.0.0.0', share= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
