{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "AutoTokenizer,\n",
    "AutoModelForCausalLM,\n",
    "BitsAndBytesConfig,\n",
    "pipeline\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, format_document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import weaviate\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689175c3da5d4d4abe8882995606db64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Loading the Mistral Model\n",
    "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Building a LLM text-generation pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1024,\n",
    "    device_map = 'auto',\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = text_generation_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(url=  'https://superteams-810p8edk.weaviate.network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    [], embedding=hf_embeddings,\n",
    "    client = client ,\n",
    "    by_text= False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simulate some document processing delay\n",
    "textsplitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.schema.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = ['/home/vardh/RAG_Mistral_Weaviate_Gradio/Leave Policy - Novus Technology.pdf']\n",
    "\n",
    "for file_path in files:\n",
    "    file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "    if file_name.lower().endswith('.pdf'):  # Check if the file is a PDF\n",
    "\n",
    "        loader_temp = PyPDFLoader(file_path)\n",
    "        docs_temp = loader_temp.load_and_split(text_splitter=textsplitter)\n",
    "        for doc in docs_temp:\n",
    "            # Replace all occurrences of '\\n' with a space ' '\n",
    "            doc.page_content = doc.page_content.replace('\\n', ' ')\n",
    "        vectorstore.add_documents(docs_temp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vectorstore.similarity_search(\"What are the policy of maternal leave?\", k= 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_content = ' '.join(doc.page_content for doc in y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'leave can be taken up to 4 weeks before the expected date of delivery. An additional 4 weeks of unpaid leave can be requested if needed. ### 4. Paternity Leave Male employees are entitled to 5 days of paid paternity leave, to be taken within 1 month of the birth of their child. ### 5. Compassionate Leave Employees are entitled to 3 days of paid compassionate leave in the event of the death of an immediate family member (spouse, child, parent, or sibling). ### 6. Unpaid Leave In exceptional circumstances, # Leave Policy ## Introduction At Novus Technologies, we recognize the importance of work-life balance and the need for employees to take time off for various reasons. This leave policy outlines the different types of leave available to employees and the procedures for requesting and managing leave. ## Types of Leave ### 1. Annual Leave All full-time employees are entitled to 20 days of paid annual leave per calendar year. Annual leave accrues on a pro-rata basis from the date of joining. Unused annual Unused annual leave can be carried forward to the next year, subject to a maximum of 10 days. ### 2. Sick Leave Employees are entitled to 10 days of paid sick leave per calendar year. Sick leave cannot be carried forward to the next year. If an employee is absent due to illness for more than three consecutive days, a medical certificate from a registered medical practitioner must be provided. ### 3. Maternity Leave Female employees are entitled to 12 weeks of paid maternity leave. This leave can be taken'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_vectordb(chatbot, msg):\n",
    "    client.schema.delete_all()\n",
    "    chatbot = \"\"\n",
    "    msg = \"\"\n",
    "    return chatbot, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_query(message, chat_history):\n",
    "    context_docs = vectorstore.similarity_search(message, k= 3)\n",
    "    context = ' '.join(doc.page_content for doc in context_docs)\n",
    "\n",
    "    template = f\"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {message}\n",
    "    \"\"\"\n",
    "\n",
    "    result = llm(template)\n",
    "\n",
    "    answer = result[\"generated_text\"].replace(template, '')\n",
    "\n",
    "    chat_history.append((message, answer))\n",
    "\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"What is the color of the sky?\\n\\nThe color of the sky varies depending on atmospheric conditions. It can appear to be various shades of blue during clear weather, red or orange during sunrise and sunset, green during a thunderstorm, or gray during an overcast day. The dominant color of the sky is blue due to the scattering of sunlight by the atmosphere.\\n\\n## What are some interesting facts about the sky?\\n\\n1. The sky is not actually a thing but rather a collective term for the gaseous atmosphere surrounding Earth and other planets.\\n2. The sky appears blue because molecules in the Earth's atmosphere scatter short-wavelength light (blue and violet) more than longer wavelengths (red, yellow, and green).\\n3. The sky is not always blue; it can take on many different colors depending on the time of day, weather conditions, and location.\\n4. The stars we see at night are actually suns just like our own Sun, but they are so far away that their light takes millions of years to reach us.\\n5. The sky is made up of several layers, including the troposphere (where we live), the stratosphere (where ozone is found), and the mesosphere (where meteors burn up).\\n6. The sky is home to many natural phenomena, such as rainbows, lightning bolts, and auroras.\\n7. The study of the sky and its phenomena is called meteorology.\\n8. The sky is constantly changing, with clouds forming and dissipating, the position of the Sun and Moon shifting throughout the day and year, and weather patterns evolving.\\n9. The sky is also home to many man-made structures, such as airplanes, satellites, and space stations.\\n10. The study of the stars and constellations in the night sky has been an important part of human history and culture for thousands of years.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"What is the color of the sky?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/routes.py:945: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/fastapi/applications.py:4495: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  return self.router.on_event(event_type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://defe2053333ace9cec.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://defe2053333ace9cec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/starlette/templating.py:172: DeprecationWarning: The `name` is not the first parameter anymore. The first parameter should be the `Request` instance.\n",
      "Replace `TemplateResponse(name, {\"request\": request})` by `TemplateResponse(request, name)`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1710, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/utils.py\", line 693, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_6280/2797907116.py\", line 8, in answer_query\n",
      "    result = chain.invoke(inputs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in <dictcomp>\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 248, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 569, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 748, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 606, in _generate_helper\n",
      "    raise e\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 593, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 266, in _generate\n",
      "    responses = self.pipeline(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 241, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1177, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1102, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 328, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1544, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2404, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1157, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1042, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 757, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 302, in forward\n",
      "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 0; 14.58 GiB total capacity; 13.48 GiB already allocated; 581.62 MiB free; 13.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/starlette/templating.py:172: DeprecationWarning: The `name` is not the first parameter anymore. The first parameter should be the `Request` instance.\n",
      "Replace `TemplateResponse(name, {\"request\": request})` by `TemplateResponse(request, name)`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/queueing.py\", line 501, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/route_utils.py\", line 258, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1710, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/blocks.py\", line 1250, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/gradio/utils.py\", line 693, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_6280/2797907116.py\", line 8, in answer_query\n",
      "    result = chain.invoke(inputs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in invoke\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2954, in <dictcomp>\n",
      "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n",
      "    return self.__get_result()\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2309, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 248, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 569, in generate_prompt\n",
      "    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 748, in generate\n",
      "    output = self._generate_helper(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 606, in _generate_helper\n",
      "    raise e\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 593, in _generate_helper\n",
      "    self._generate(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/langchain_community/llms/huggingface_pipeline.py\", line 266, in _generate\n",
      "    responses = self.pipeline(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 241, in __call__\n",
      "    return super().__call__(text_inputs, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1177, in __call__\n",
      "    outputs = list(final_iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py\", line 125, in __next__\n",
      "    processed = self.infer(item, **self.params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1102, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/pipelines/text_generation.py\", line 328, in _forward\n",
      "    generated_sequence = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1544, in generate\n",
      "    return self.greedy_search(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2404, in greedy_search\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1157, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1042, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 757, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 285, in forward\n",
      "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.68 GiB (GPU 0; 14.58 GiB total capacity; 10.96 GiB already allocated; 581.62 MiB free; 13.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "\n",
    "   with gr.Row():\n",
    "       upload_files = gr.File(label= \"Upload pdf files only\", file_count= 'multiple')\n",
    "       success_msg = gr.Text(value=\"\")\n",
    "\n",
    "   chatbot = gr.Chatbot()\n",
    "   msg = gr.Textbox(label= \"Enter your query here\")\n",
    "   clear = gr.ClearButton([msg, chatbot], value= \"Clear VectorDB\")\n",
    "\n",
    "\n",
    "   upload_files.upload(add_pdfs_to_vectorstore, upload_files, success_msg)\n",
    "   msg.submit(answer_query, [msg, chatbot], [msg, chatbot])\n",
    "\n",
    "demo.launch(server_name='0.0.0.0', share= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
